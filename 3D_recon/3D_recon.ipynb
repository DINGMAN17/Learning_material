{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3D_recon.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNwaZfqAJTpDWctMiVtZQxm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cftgbJrGQ1_C"},"source":["<!--NOTEBOOK_HEADER-->\n","*This notebook contains material from the textbook [Multiple View Geometry in computer Vision](https://books.google.com.sg/books/about/Multiple_View_Geometry_in_Computer_Visio.html?id=si3R3Pfa98QC&source=kp_book_description&redir_esc=y) and lecture notes [Standford CS231A ](http://web.stanford.edu/class/cs231a/);\n","content is available [on Github](https://github.com/DINGMAN17/Learning_material/tree/main/3D_recon).*"]},{"cell_type":"markdown","metadata":{"id":"kqn7Xg2yfMvV"},"source":["**Please install the necessary packages to run the code**"]},{"cell_type":"code","metadata":{"id":"NQsMXzGrwvGA"},"source":["%pip install scipy numpy opencv-python"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMFR32ZRwwIT"},"source":["# Multi-view 3D reconstruction code demo"]},{"cell_type":"markdown","metadata":{"id":"-xLbb4I5Xduh"},"source":["##Table of contents:\n","---\n","**1.   Camera basics**\n","\n","**2.   Epipolar Geometry**\n"]},{"cell_type":"markdown","metadata":{"id":"UdoFo3USPRIJ"},"source":["## 1 Camera basics\n","\n","**1.1 Camera Matrix Model**\n","\n","**1.2 Camera calibration**\n","\n","**1.3 Camera Distortion**\n","\n","**1.4 Code demo 1: Camera calibration for distortion correction**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lBQp5lUeVhfM"},"source":["### ***1.1 Camera Matrix Model***\n","\n","Camera Matrix Model describes a set of important parameters that affect how a **world point $P$ is mapped to image coordinates $P'$**.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1A4exQsK6Q9B"},"source":["\n","#### ***1.1.1 Intrinsic parameters***\n","\n","*   Translation vector $[c_x\\ c_y]^T$: describe how \n","image plane and digital image coordinates can differ by a translation\n","*   Change of units $k$ and $l$: digital images and image plane are represented in different units, one in pixels and one in physical measurements\n","\n","*   Skewness $\\theta$: caused by sensor manufacturing errors\n","*   [Distortion](https://en.wikipedia.org/wiki/Distortion_%28optics%29) (ignore for now)\n","\n","Make use of [Homogeneous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates#:~:text=Any%20point%20in%20the%20projective,multiplied%20by%20a%20common%20factor), a point in 3D space and its image coordinates by a matrix vector relationship can be expressed as:\n","$$P' =\n","\\begin{bmatrix}\n","\\alpha & -\\alpha\\cot\\theta & c_x\\\\\n","0 & \\frac{\\beta}{\\sin\\theta} & c_y\\\\\n","0 & 0 & 1\n","\\end{bmatrix}\\begin{bmatrix}\n","I & 0\\\\\n","\\end{bmatrix}P = K\\begin{bmatrix}\n","I & 0\\\\\n","\\end{bmatrix}P$$\n","\n","The matrix $K$ is often referred to as the **camera matrix**\n","$$ K = \\begin{bmatrix}\n","x'\\\\y'\\\\z\\\\\n","\\end{bmatrix}=\\begin{bmatrix}\n","\\alpha & -\\alpha\\cot\\theta & c_x\\\\\n","0 & \\frac{\\beta}{\\sin\\theta} & c_y\\\\\n","0 & 0 & 1\n","\\end{bmatrix}$$\n","\n","Camera matrix $K$ has **5 degrees of freedom**: 2 for focal length, 2 for offset, and 1 for skewness. (assume no distortion). $K$ is unique and inherent to a given camera."]},{"cell_type":"markdown","metadata":{"id":"Ip-Q9shO6GS8"},"source":["#### ***1.1.2 Extrinsic Parameters***\n","The above mapping is between a point $P'$ in the **3D camera reference system** to a point $P$ in the **2D image plane**. However, the information about the 3D world may be available in a different coordinate system. Hence, additional transformation captured by **rotation matrix $R$** and **translation vector $T$** is introduced to relate points from the **world reference system** to the **3D camera reference system**\n","\n","Given a point in a world reference system $P_w$, we can compute its camera coordinates:\n","$$P'=K\\begin{bmatrix}\n","R & T\\\\\n","\\end{bmatrix}P_w=MP_w$$\n","\n","Matrices $R$ and $T$ are known as the **extrinsic parameters** as do not depend on the camera.\n","\n","With extrinsic and intrinsic parameters $M$, mapping from a 3D point P in an arbitrary world\n","reference system to the image plane can hence be achieved.\n","\n","In total, the projection matrix $M$ has **$11$ degrees of freedom**: 5 from the intrinsic camera matrix,\n","3 from extrinsic rotation, and 3 from extrinsic translation.\n"]},{"cell_type":"markdown","metadata":{"id":"twuoHZVXAfJI"},"source":["### ***1.2 Camera calibration***\n","**Purpose:** Estimate the extrinsic and intrinsic camera parameters.\n","\n","**How:** Solve for the intrinsic camera matrix $K$ and\n","the extrinsic parameters $R, T$\n","\n","**Setup:** Provide calibration rig which consists of a simple pattern, $i.e.$ checkerboard) with known dimensions. the rig defines our world reference frame with origin $O_w$ and axes $i_w,\\ j_w,\\ k_w$. From the rig's known pattern, we have known points\n","in the world reference frame $P_1,..., P_n$. Finding these points in the image we\n","take from the camera gives corresponding points in the image $p_1,..., p_n$.. \n","<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/Capture.PNG' alt=\"Setup\">\n","<figcaption>Figure1: The setup of an example calibration rig</figcaption></center>\n","</figure>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qIjcQ3O0BeoZ"},"source":["### **1.3 Camera distortion**"]},{"cell_type":"markdown","metadata":{"id":"_n7VieRsBtQm"},"source":["### **1.4 Code demo 1: Camera calibration for distortion correction**"]},{"cell_type":"markdown","metadata":{"id":"H-Z7vH3OYeG8"},"source":["## **2 Epipolar Geometry**"]},{"cell_type":"markdown","metadata":{"id":"NJuF6YWoNHVP"},"source":["#### How is the Fundamental matrix useful:\n","if we know the Fundamental matrix, then simply knowing a point in an image\n","gives us an easy constraint of the corresponding point in the other image. \n","\n","Therefore, without knowing the actual position of $P$ in $3D$ space, or any of the extrinsic or intrinsic characteristics of the cameras, we\n","can establish a relationship between any $P$ and $P_0$."]},{"cell_type":"markdown","metadata":{"id":"J0FLjhL8Y_aE"},"source":["Algorithm\n","\n","*   The Eight-Point Algorithm\n","*   The Normalized Eight-Point Algorithm"]},{"cell_type":"markdown","metadata":{"id":"CZhNiUAceh0z"},"source":["<a id='8_point'></a>\n","### 2.1 The Eight-Point Algorithm"]},{"cell_type":"markdown","metadata":{"id":"PaIvE60ce3IJ"},"source":["<a id='normalized_8'></a>\n","### 2.2 The Normalized Eight-Point Algorithm"]}]}