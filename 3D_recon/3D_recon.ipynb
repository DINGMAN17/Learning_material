{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3D_recon.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQpuwZt6KvWp9XRjYbqsY3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DINGMAN17/Learning_material/blob/main/3D_recon/3D_recon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cftgbJrGQ1_C"
      },
      "source": [
        "<!--NOTEBOOK_HEADER-->\n",
        "*This notebook contains material from the textbook [Multiple View Geometry in computer Vision](https://books.google.com.sg/books/about/Multiple_View_Geometry_in_Computer_Visio.html?id=si3R3Pfa98QC&source=kp_book_description&redir_esc=y) and lecture notes [Standford CS231A ](http://web.stanford.edu/class/cs231a/);\n",
        "content is available [on Github](https://github.com/DINGMAN17/Learning_material/tree/main/3D_recon)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqn7Xg2yfMvV"
      },
      "source": [
        "**Please install the necessary packages to run the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQsMXzGrwvGA"
      },
      "source": [
        "%pip install scipy numpy opencv-python tqdm glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMFR32ZRwwIT"
      },
      "source": [
        "# Multi-view 3D reconstruction code demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xLbb4I5Xduh"
      },
      "source": [
        "##Table of contents:\n",
        "---\n",
        "**1.   Camera basics**\n",
        "\n",
        "**2.   Epipolar Geometry**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdoFo3USPRIJ"
      },
      "source": [
        "## 1 Camera basics\n",
        "\n",
        "**1.1 Camera Matrix Model**\n",
        "\n",
        "**1.2 Camera calibration**\n",
        "\n",
        "**1.3 Camera Distortion**\n",
        "\n",
        "**1.4 Code demo 1: Camera calibration for distortion correction**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBQp5lUeVhfM"
      },
      "source": [
        "### ***1.1 Camera Matrix Model***\n",
        "\n",
        "Camera Matrix Model describes a set of important parameters that affect how a **world point $P$ is mapped to image coordinates $P'$**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A4exQsK6Q9B"
      },
      "source": [
        "\n",
        "#### ***1.1.1 Intrinsic parameters***\n",
        "\n",
        "*   Translation vector $[c_x\\ c_y]^T$: describe how \n",
        "image plane and digital image coordinates can differ by a translation\n",
        "*   Change of units $k$ and $l$: digital images and image plane are represented in different units, one in pixels and one in physical measurements\n",
        "\n",
        "*   Skewness $\\theta$: caused by sensor manufacturing errors\n",
        "*   [Distortion](https://en.wikipedia.org/wiki/Distortion_%28optics%29) (ignore for now)\n",
        "\n",
        "Make use of [Homogeneous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates#:~:text=Any%20point%20in%20the%20projective,multiplied%20by%20a%20common%20factor), a point in 3D space and its image coordinates by a matrix vector relationship can be expressed as:\n",
        "$$P' =\n",
        "\\begin{bmatrix}\n",
        "\\alpha & -\\alpha\\cot\\theta & c_x\\\\\n",
        "0 & \\frac{\\beta}{\\sin\\theta} & c_y\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "I & 0\\\\\n",
        "\\end{bmatrix}P = K\\begin{bmatrix}\n",
        "I & 0\\\\\n",
        "\\end{bmatrix}P$$\n",
        "\n",
        "The matrix $K$ is often referred to as the **camera matrix**\n",
        "$$ K = \\begin{bmatrix}\n",
        "x'\\\\y'\\\\z\\\\\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "\\alpha & -\\alpha\\cot\\theta & c_x\\\\\n",
        "0 & \\frac{\\beta}{\\sin\\theta} & c_y\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Camera matrix $K$ has **5 degrees of freedom**: 2 for focal length, 2 for offset, and 1 for skewness. (assume no distortion). $K$ is unique and inherent to a given camera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-Q9shO6GS8"
      },
      "source": [
        "#### ***1.1.2 Extrinsic Parameters***\n",
        "The above mapping is between a point $P'$ in the **3D camera reference system** to a point $P$ in the **2D image plane**. However, the information about the 3D world may be available in a different coordinate system. Hence, additional transformation captured by **rotation matrix $R$** and **translation vector $T$** is introduced to relate points from the **world reference system** to the **3D camera reference system**\n",
        "\n",
        "Given a point in a world reference system $P_w$, we can compute its camera coordinates:\n",
        "$$P'=K\\begin{bmatrix}\n",
        "R & T\\\\\n",
        "\\end{bmatrix}P_w=MP_w$$\n",
        "\n",
        "Matrices $R$ and $T$ are known as the **extrinsic parameters** as do not depend on the camera.\n",
        "\n",
        "With extrinsic and intrinsic parameters $M$, mapping from a 3D point P in an arbitrary world\n",
        "reference system to the image plane can hence be achieved.\n",
        "\n",
        "In total, the projection matrix $M$ has **$11$ degrees of freedom**: 5 from the intrinsic camera matrix,\n",
        "3 from extrinsic rotation, and 3 from extrinsic translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twuoHZVXAfJI"
      },
      "source": [
        "### ***1.2 Camera calibration***\n",
        "**Purpose:** Estimate the extrinsic and intrinsic camera parameters.\n",
        "\n",
        "**How:** Solve for the intrinsic camera matrix $K$ and\n",
        "the extrinsic parameters $R, T$\n",
        "\n",
        "**Setup:** Provide calibration rig which consists of a simple pattern, $i.e.$ checkerboard) with known dimensions. the rig defines our world reference frame with origin $O_w$ and axes $i_w,\\ j_w,\\ k_w$. From the rig's known pattern, we have known points\n",
        "in the world reference frame $P_1,..., P_n$. Finding these points in the image we\n",
        "take from the camera gives corresponding points in the image $p_1,..., p_n$.. \n",
        "<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/calib_setup.PNG' alt=\"Setup\">\n",
        "<figcaption>Figure1: The setup of an example calibration rig</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Hence, a linear system of equations from $n$ correspondences can be set up such that for each correspondence $P_i$, $p_i$ and camera matrix $M$ whose rows are $m_1,m_2,m_3$:\n",
        "\n",
        "$$p_i=\\begin{bmatrix}\n",
        "u_i \\\\ v_i\\\\\n",
        "\\end{bmatrix}=MP_i=\\begin{bmatrix}\n",
        "\\frac{m_1p_i}{m_3p_i} \\\\ \\frac{m_2p_i}{m_3p_i}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "which is equivalent of solving a pair of equations:\n",
        "$$u_i(m_3P_i)-m_1P_i=0$$\n",
        "$$v_i(m_3P_i)-m_2P_i=0$$\n",
        "\n",
        "Here, there are $2$ constraints for solving the unknown parameters contained in $m$. From 1.1.2, we know that the **camera matrix $M$ has 11 unknown parameters**. This suggests that we need **at least $6$ correspondences** to solve this.\n",
        "\n",
        "**Note:** Not all sets of $n$ correspondences will work. For example, if the points $Pi$ lie on the same plane, then the system will not be able to be solved. These unsolvable configurations of points are known as **degenerate configurations**.\n",
        "\n",
        "In the real world applications (such as the demo in 1.4), we often use more than 6 points as measurements are often noisy. When $2n > 11$, the above homogeneous linear system is overdetermined. Therefore, it can be treated as a minimization problem and can be solved using [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIjcQ3O0BeoZ"
      },
      "source": [
        "### **1.3 Camera distortion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n7VieRsBtQm"
      },
      "source": [
        "### **1.4 Code demo 1: Camera calibration for distortion correction**\n",
        "\n",
        "This code demo is written in [Python](https://www.python.org/) which uses pre-defined functions from [OpenCV](https://opencv.org/) library. The reference can be found [here](https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html). \n",
        "\n",
        "The distorted images used in this demo were taken by a thermal-RGB camera for drone operations, the detail specs can be found [here](https://www.dji.com/sg/zenmuse-xt2).\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon//sample_rgb_distort.jpg' alt=\"sample distorted rgb image\">\n",
        "<figcaption>Figure2: sample distorted rgb image</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "**Interesting application:** It is possible to calibrate thermal images as well using a spcially fabricated chessboard made of aluminium foil. Unlike RGB images, pre-processing is required to extract the temperature and set thresholds for the thermal images. As shown in the two images below, raw thermal image is on the left and the processed image is on the right.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/sample_thermal_distort.JPG' width=\"425\"/> <img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/sample_thermal_theshold.JPG' width=\"425\"/> \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYD7a8SbdacH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J9wi4TU7ri"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91VSTMNKUfQP"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import tqdm\n",
        "import cv2 as cv\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vBAFskaVCCR"
      },
      "source": [
        "def get_camera_params(number_rows,number_cols,input_folder,print_params=False):\n",
        "    '''\n",
        "    get camera parameters based on a set of chessboard images (>10 images)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    number_rows, number_cols : int, point number of rows/columns of chessboard\n",
        "    input_folder : str\n",
        "    print_params : boolean, optional\n",
        "                  whether to print the camera parameters\n",
        "    '''\n",
        "    # findChessBoard flags\n",
        "    find_flags = cv.CALIB_CB_ADAPTIVE_THRESH + cv.CALIB_CB_NORMALIZE_IMAGE + \\\n",
        "    \t\t\t        cv.CALIB_CB_FILTER_QUADS + cv.CALIB_CB_FAST_CHECK\n",
        "       \n",
        "    # termination criteria \n",
        "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "       \n",
        "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
        "    objp = np.zeros((number_cols*number_rows, 3), np.float32)\n",
        "    objp[:,:2] = np.mgrid[0:number_rows, 0:number_cols].T.reshape(-1,2)\n",
        "    \n",
        "    objpoints = [] # 3d point in real world space\n",
        "    imgpoints = [] # 2d points in image plane.\n",
        "    \n",
        "    # read images\n",
        "    images = glob.glob(os.path.join(input_folder, '*.JPG')) \n",
        "    fail_all = True\n",
        "\n",
        "    for fname in tqdm(images):\n",
        "        img = cv.imread(fname)\n",
        "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)    \n",
        "\n",
        "        # Find the chess board corners\n",
        "        ret,corners = cv.findChessboardCorners(gray,(number_rows,number_cols),\n",
        "                                               find_flags)\n",
        "    \n",
        "        # If found, add object points, image points (after refining them)    \n",
        "        if ret == True:\n",
        "            fail_all = False\n",
        "            objpoints.append(objp)\n",
        "            # improve accuracy\n",
        "            corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
        "            imgpoints.append(corners2)\n",
        "            \n",
        "            # Draw the corners \n",
        "            cv.drawChessboardCorners(img,(number_rows,number_cols),corners2,ret)\n",
        "\n",
        "            # save images with corner points\n",
        "            raw_img = os.path.split(fname)     \n",
        "            img_name = 'draw_'+raw_img[-1]   \n",
        "            cv.imwrite(os.path.join(input_folder, img_name), img)\n",
        "           \n",
        "            # display succeed or fail to find corners\n",
        "            print(fname, ': succeed to find corner')\n",
        "        else:\n",
        "            print(fname, ': fail')\n",
        "        \n",
        "    # Calibration camera to find intrinsic, extrinsic parameters\n",
        "    if fail_all == False:\n",
        "        imageSize = gray.shape[::-1]\n",
        "        err,KK,distCoeffs,rvecs,tvecs = cv.calibrateCamera(objpoints,imgpoints,\n",
        "                                                           imageSize,None,None)\n",
        "        \n",
        "        # Save the camera parameters\n",
        "        parameter_arry = np.array([err, KK, distCoeffs, rvecs, tvecs])\n",
        "        np.save('calibration_parameters.npy', parameter_arry)\n",
        "        \n",
        "        # # Print calibaration parameters\n",
        "        if print_params:\n",
        "            print('err', err)\n",
        "            print('KK', KK)\n",
        "            print('distCoeffs', distCoeffs)\n",
        "            print('rvecs', rvecs)\n",
        "            print('tvecs', tvecs)\n",
        "        \n",
        "        # Calculate re-projection error\n",
        "        mean_error = 0\n",
        "        for i in range(len(objpoints)):\n",
        "            imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i],\n",
        "                                             tvecs[i], KK, distCoeffs)\n",
        "            error = cv.norm(imgpoints[i],imgpoints2,cv.NORM_L2)/len(imgpoints2)\n",
        "            mean_error += error\n",
        "        print(\"total error: \", mean_error/len(objpoints))\n",
        "        error_list = [['mean_error',mean_error],['len(objpoints)',len(objpoints)],\n",
        "                      [\"total error: \", mean_error/len(objpoints)]]\n",
        "        np.savetxt('Re_projection_error.csv',error_list,delimiter=',',fmt ='% s')\n",
        "        return err, KK, distCoeffs, rvecs, tvecs\n",
        "    else:\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Z7vH3OYeG8"
      },
      "source": [
        "## **2 Epipolar Geometry**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJuF6YWoNHVP"
      },
      "source": [
        "#### How is the Fundamental matrix useful:\n",
        "if we know the Fundamental matrix, then simply knowing a point in an image\n",
        "gives us an easy constraint of the corresponding point in the other image. \n",
        "\n",
        "Therefore, without knowing the actual position of $P$ in $3D$ space, or any of the extrinsic or intrinsic characteristics of the cameras, we\n",
        "can establish a relationship between any $P$ and $P_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0FLjhL8Y_aE"
      },
      "source": [
        "Algorithm\n",
        "\n",
        "*   The Eight-Point Algorithm\n",
        "*   The Normalized Eight-Point Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZhNiUAceh0z"
      },
      "source": [
        "<a id='8_point'></a>\n",
        "### 2.1 The Eight-Point Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIvE60ce3IJ"
      },
      "source": [
        "<a id='normalized_8'></a>\n",
        "### 2.2 The Normalized Eight-Point Algorithm"
      ]
    }
  ]
}