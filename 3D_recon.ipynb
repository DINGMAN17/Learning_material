{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3D_recon.ipynb",
      "provenance": [],
      "mount_file_id": "18Fa2iN7LfnzzZVc3zwZ1cXyAujSMVmsv",
      "authorship_tag": "ABX9TyMIVIyxwC+IrwtYt/Nz6fMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DINGMAN17/Learning_material/blob/main/3D_recon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cftgbJrGQ1_C"
      },
      "source": [
        "<!--NOTEBOOK_HEADER-->\n",
        "This notebook contains material from the textbook [Multiple View Geometry in computer Vision](https://books.google.com.sg/books/about/Multiple_View_Geometry_in_Computer_Visio.html?id=si3R3Pfa98QC&source=kp_book_description&redir_esc=y) and lecture notes [Standford CS231A ](http://web.stanford.edu/class/cs231a/);\n",
        "content is available [on Github](https://github.com/DINGMAN17/Learning_material/tree/main/3D_recon)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXrnTA68QA-o",
        "outputId": "94662ef5-eeb4-45ac-a7b4-13ee11951b4b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqn7Xg2yfMvV"
      },
      "source": [
        "**Please install the necessary packages to run the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQsMXzGrwvGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5c016d-627a-46ae-c24f-d5431323cdaa"
      },
      "source": [
        "%pip install scipy numpy opencv-python tqdm "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMFR32ZRwwIT"
      },
      "source": [
        "# Multi-view 3D reconstruction code demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xLbb4I5Xduh"
      },
      "source": [
        "##Table of contents:\n",
        "---\n",
        "**1.   Camera basics**\n",
        "\n",
        "**2.   Epipolar Geometry**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdoFo3USPRIJ"
      },
      "source": [
        "## 1 Camera basics\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**1.1 Camera Matrix Model**\n",
        "\n",
        "**1.2 Camera calibration**\n",
        "\n",
        "**1.3 Camera Distortion**\n",
        "\n",
        "**1.4 Code demo 1: Camera calibration for distortion correction**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBQp5lUeVhfM"
      },
      "source": [
        "### ***1.1 Camera Matrix Model***\n",
        "\n",
        "Camera Matrix Model describes a set of important parameters that affect how a **world point $P$ is mapped to image coordinates $P'$**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A4exQsK6Q9B"
      },
      "source": [
        "\n",
        "#### ***1.1.1 Intrinsic parameters***\n",
        "\n",
        "*   Translation vector $[c_x\\ c_y]^T$: describe how \n",
        "image plane and digital image coordinates can differ by a translation\n",
        "*   Change of units $k$ and $l$: digital images and image plane are represented in different units, one in pixels and one in physical measurements\n",
        "\n",
        "*   Skewness $\\theta$: caused by sensor manufacturing errors\n",
        "*   [Distortion](https://en.wikipedia.org/wiki/Distortion_%28optics%29) (ignore for now, cover in 1.3)\n",
        "\n",
        "Make use of [Homogeneous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates#:~:text=Any%20point%20in%20the%20projective,multiplied%20by%20a%20common%20factor), a point in 3D space and its image coordinates by a matrix vector relationship can be expressed as:\n",
        "$$P' =\n",
        "\\begin{bmatrix}\n",
        "\\alpha & -\\alpha\\cot\\theta & c_x\\\\\n",
        "0 & \\frac{\\beta}{\\sin\\theta} & c_y\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "I & 0\\\\\n",
        "\\end{bmatrix}P = K\\begin{bmatrix}\n",
        "I & 0\\\\\n",
        "\\end{bmatrix}P$$\n",
        "\n",
        "The matrix $K$ is often referred to as the **camera matrix**\n",
        "$$ K = \\begin{bmatrix}\n",
        "x'\\\\y'\\\\z\\\\\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "\\alpha & -\\alpha\\cot\\theta & c_x\\\\\n",
        "0 & \\frac{\\beta}{\\sin\\theta} & c_y\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Camera matrix $K$ has **5 degrees of freedom**: 2 for focal length, 2 for offset, and 1 for skewness. (assume no distortion). $K$ is unique and inherent to a given camera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-Q9shO6GS8"
      },
      "source": [
        "#### ***1.1.2 Extrinsic Parameters***\n",
        "The above mapping is between a point $P'$ in the **3D camera reference system** to a point $P$ in the **2D image plane**. However, the information about the 3D world may be available in a different coordinate system. Hence, additional transformation captured by **rotation matrix $R$** and **translation vector $T$** is introduced to relate points from the **world reference system** to the **3D camera reference system**\n",
        "\n",
        "Given a point in a world reference system $P_w$, we can compute its camera coordinates:\n",
        "$$P'=K\\begin{bmatrix}\n",
        "R & T\\\\\n",
        "\\end{bmatrix}P_w=MP_w$$\n",
        "\n",
        "Matrices $R$ and $T$ are known as the **extrinsic parameters** as do not depend on the camera.\n",
        "\n",
        "With extrinsic and intrinsic parameters $M$, mapping from a 3D point P in an arbitrary world\n",
        "reference system to the image plane can hence be achieved.\n",
        "\n",
        "In total, the projection matrix $M$ has **$11$ degrees of freedom**: 5 from the intrinsic camera matrix,\n",
        "3 from extrinsic rotation, and 3 from extrinsic translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twuoHZVXAfJI"
      },
      "source": [
        "### ***1.2 Camera calibration***\n",
        "**Purpose:** Estimate the extrinsic and intrinsic camera parameters.\n",
        "\n",
        "**How:** Solve for the intrinsic camera matrix $K$ and\n",
        "the extrinsic parameters $R, T$\n",
        "\n",
        "**Setup:** Provide calibration rig which consists of a simple pattern, $i.e.$ checkerboard) with known dimensions. the rig defines our world reference frame with origin $O_w$ and axes $i_w,\\ j_w,\\ k_w$. From the rig's known pattern, we have known points\n",
        "in the world reference frame $P_1,..., P_n$. Finding these points in the image we\n",
        "take from the camera gives corresponding points in the image $p_1,..., p_n$\n",
        "<figure>\n",
        "<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/calib_setup.PNG' alt=\"Setup\">\n",
        "<figcaption>Figure1: The setup of an example calibration rig</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Hence, a linear system of equations from $n$ correspondences can be set up such that for each correspondence $P_i$, $p_i$ and camera matrix $M$ whose rows are $m_1,m_2,m_3$:\n",
        "\n",
        "$$p_i=\\begin{bmatrix}\n",
        "u_i \\\\ v_i\\\\\n",
        "\\end{bmatrix}=MP_i=\\begin{bmatrix}\n",
        "\\frac{m_1p_i}{m_3p_i} \\\\ \\frac{m_2p_i}{m_3p_i}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "which is equivalent of solving a pair of equations:\n",
        "$$u_i(m_3P_i)-m_1P_i=0$$\n",
        "$$v_i(m_3P_i)-m_2P_i=0$$\n",
        "\n",
        "Here, there are $2$ constraints for solving the unknown parameters contained in $m$. From 1.1.2, we know that the **camera matrix $M$ has 11 unknown parameters**. This suggests that we need **at least $6$ correspondences** to solve this.\n",
        "\n",
        "**Note:** Not all sets of $n$ correspondences will work. For example, if the points $Pi$ lie on the same plane, then the system will not be able to be solved. These unsolvable configurations of points are known as **degenerate configurations**.\n",
        "\n",
        "In the real world applications (such as the demo in 1.4), we often use more than 6 points as measurements are often noisy. When $2n > 11$, the above homogeneous linear system is overdetermined. Therefore, it can be treated as a minimization problem and can be solved using [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIjcQ3O0BeoZ"
      },
      "source": [
        "### **1.3 Camera distortion**\n",
        "\n",
        "So far, we have been working with ideal lenses which are free from any distortion. However, as seen before, real lenses can deviate from rectilinear projection, which require more advanced methods to un-distort the images. \n",
        "\n",
        "Two major kinds of distortion are **radial distortion** and **tangential distortion**.\n",
        "\n",
        "Radial distortion causes straight lines to appear curved. Radial distortion becomes larger the farther points are from the center of the image, shown on the left. While tangential distortion occurs because the image-taking lense is not aligned perfectly parallel to the imaging plane. So, some areas in the image may look nearer than expected.\n",
        "\n",
        "<figure>\n",
        "<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/distortion.jpg' alt=\"Distortion illustration\">\n",
        "<figcaption>Figure2: radial distortion and tangential distortion</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "- Radial distortion can be represented as follows:\n",
        "$$x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)$$\n",
        "$$y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)$$\n",
        "\n",
        "- Tangential distortion can be represented as follows:\n",
        "$$x_{distorted}=x+[2p_1xy+p_2(r^2+2x^2)]$$\n",
        "$$y_{distorted}=y+[2p_2xy+p_1(r^2+2y^2)]$$\n",
        "\n",
        "In total, distortion coefficients consist of five unknown parameters:\n",
        "$$Distortion\\ coefficients=(k_1\\ k_2\\ k_3\\ p_1\\ p_2)$$\n",
        "\n",
        "In the next section, we will see how to find the distortion coefficients and correct the distortion using some sample images of a well defined pattern (e.g. a chess board).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n7VieRsBtQm"
      },
      "source": [
        "### **1.4 Code demo 1: Camera calibration for distortion correction**\n",
        "\n",
        "This code demo is written in [Python](https://www.python.org/) which uses pre-defined functions from [OpenCV](https://opencv.org/) library. The reference can be found [here](https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html). \n",
        "\n",
        "The distorted images used in this demo were taken by a thermal-RGB camera for drone operations, the detail specs can be found [here](https://www.dji.com/sg/zenmuse-xt2). We will only focus on RGB images in this task. \n",
        "\n",
        "<figure>\n",
        "<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon//sample_rgb_distort.jpg' alt=\"sample distorted rgb image\">\n",
        "<figcaption>Figure2: Sample distorted rgb image</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J9wi4TU7ri"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91VSTMNKUfQP"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import tqdm\n",
        "import cv2 as cv\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHIpPSwtetG"
      },
      "source": [
        "A function `get_camera_params` has been written for you to execute the above steps altogether. Instead of inputing just one image, we will use 11 RGB raw images inside folder `rgb_raw` to get a better estimation of camera parameters. \n",
        "\n",
        "Run the two code cells below. Note that it takes roughly 5 mins to complete the process. \n",
        "\n",
        "This function does the following:\n",
        "\n",
        "*   Prepare 3D real world points (object points) and the corresponding 2D coordinates (image points) of these points in each image by finding pattern in chess board using [`cv.findChessboardCorners()`](https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a)\n",
        "\n",
        "*   Increase the accuracy of the points using [`cv.cornerSubPix()`](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e) and draw the pattern using [`cv.drawChessboardCorners()`](https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga6a10b0bb120c4907e5eabbcd22319022).\n",
        "*   Calibrate the camera using [`cv.calibrateCamera()`](https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d) which returns the camera matrix $K$, distortion coefficients, rotation and translation vectors $R, T$\n",
        "*   Calculate re-projection error which gives a good estimation of how exact the found parameters are. This is done throught transforming the object point to image point using [`cv.projectPoints()`](https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) and calculate the absolute norm between the transformation and the corner finding algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vBAFskaVCCR"
      },
      "source": [
        "def get_camera_params(number_rows,number_cols,input_folder):\n",
        "    '''\n",
        "    get camera parameters based on a set of chessboard images (>10 images)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    number_rows, number_cols : int, point number of rows/columns of chessboard\n",
        "    input_folder : str\n",
        "    '''\n",
        "    # findChessBoard flags\n",
        "    find_flags = cv.CALIB_CB_ADAPTIVE_THRESH + cv.CALIB_CB_NORMALIZE_IMAGE + \\\n",
        "    \t\t\t        cv.CALIB_CB_FILTER_QUADS + cv.CALIB_CB_FAST_CHECK\n",
        "       \n",
        "    # Define termination criteria for corner fingding algorithm\n",
        "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "       \n",
        "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
        "    objp = np.zeros((number_cols*number_rows, 3), np.float32)\n",
        "    objp[:,:2] = np.mgrid[0:number_rows, 0:number_cols].T.reshape(-1,2)\n",
        "    \n",
        "    objpoints = [] # 3d point in real world space\n",
        "    imgpoints = [] # 2d points in image plane.\n",
        "    \n",
        "    # read images\n",
        "    images = glob.glob(os.path.join(input_folder, '*.jpg'))+ \\\n",
        "              glob.glob(os.path.join(input_folder, '*.JPG'))\n",
        "\n",
        "    fail_all = True\n",
        "    for fname in tqdm.tqdm(images):\n",
        "  \n",
        "        img = cv.imread(fname)\n",
        "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)    \n",
        "\n",
        "        # Find the chess board corners\n",
        "        ret,corners = cv.findChessboardCorners(gray,(number_rows,number_cols),\n",
        "                                               find_flags)\n",
        "    \n",
        "        # If found, add object points, image points (after refining them)    \n",
        "        if ret == True:\n",
        "            fail_all = False\n",
        "            objpoints.append(objp)\n",
        "            # improve accuracy\n",
        "            corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
        "            imgpoints.append(corners2)\n",
        "            \n",
        "            # Draw the corners \n",
        "            cv.drawChessboardCorners(img,(number_rows,number_cols),corners2,ret)\n",
        "\n",
        "            # save images with corner points\n",
        "            raw_img = os.path.split(fname)     \n",
        "            img_name = 'draw_'+raw_img[-1]   \n",
        "            cv.imwrite(os.path.join(input_folder, img_name), img)\n",
        "           \n",
        "            # display succeed or fail to find corners\n",
        "            print(fname, ': succeed to find corner')\n",
        "        else:\n",
        "            print(fname, ': fail')\n",
        "        \n",
        "    # Calibration camera to find intrinsic, extrinsic parameters\n",
        "    if fail_all == False:\n",
        "        imageSize = gray.shape[::-1]\n",
        "        err,KK,distCoeffs,rvecs,tvecs = cv.calibrateCamera(objpoints,imgpoints,\n",
        "                                                           imageSize,None,None)\n",
        "\n",
        "        # Calculate re-projection error\n",
        "        total_error = 0\n",
        "        for i in range(len(objpoints)):\n",
        "            imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i],\n",
        "                                             tvecs[i], KK, distCoeffs)\n",
        "            error = cv.norm(imgpoints[i],imgpoints2,cv.NORM_L2)/len(imgpoints2)\n",
        "            total_error += error\n",
        "        mean_err = total_error/len(objpoints)\n",
        "        print(\"Re-projection error: \", mean_err)\n",
        "        return err, KK, distCoeffs, rvecs, tvecs\n",
        "    else:\n",
        "        return"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JUQJDzEbZif",
        "outputId": "9551bd80-5b8e-4f7c-ad6c-34c133636cb3"
      },
      "source": [
        "err,K,distCoeffs,rvecs,tvecs = get_camera_params(9,7,'/content/drive/MyDrive/3D_recon/rgb_raw')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/3D_recon/rgb_raw/DJI_0036.jpg', '/content/drive/MyDrive/3D_recon/rgb_raw/DJI_0042.jpg', '/content/drive/MyDrive/3D_recon/rgb_raw/DJI_0038.jpg', '/content/drive/MyDrive/3D_recon/rgb_raw/draw_DJI_0036.jpg', '/content/drive/MyDrive/3D_recon/rgb_raw/draw_DJI_0042.jpg', '/content/drive/MyDrive/3D_recon/rgb_raw/draw_DJI_0038.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znc9TtIHhbYI",
        "outputId": "02e9d02b-1739-46b7-ed16-ddae8d56be3e"
      },
      "source": [
        "#if you are interested to check out the camera parameters, uncomment the \n",
        "#parameters to print them out.\n",
        "\n",
        "print(\"Re-projection error:\", err)\n",
        "#print(\"Camera matrix: \", K)\n",
        "#print(\"Distortion coefficients: \", distCoeffs)\n",
        "#print(\"Rotation matrix\", rvecs)\n",
        "#print(\"Translation matrix\", tvecs)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Re-projection error: 0.9742068538746896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JD-aKWGjOL-"
      },
      "source": [
        "**Add-on challenge:** It is possible to calibrate thermal camera as well using a spcially fabricated chessboard made of aluminium foil. Unlike RGB images, image pre-processing is required to extract the temperature and to set thresholds for the thermal images, as shown below. I have provided a folder named `IR_raw` with 12 pre-processed thermal images. See if you can calibrate the thermal camera by running the function `get_camera_params`. *Hint*: change the size of the chessboard. \n",
        "\n",
        "<img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/sample_thermal_distort.JPG' width=\"425\"/> <img src='https://raw.githubusercontent.com/DINGMAN17/Learning_material/main/3D_recon/sample_thermal_theshold.JPG' width=\"425\"/> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5T569b5jtfH"
      },
      "source": [
        "Instruction: fill in the correct value for the two variables `number_rows` and `number_cols`. Uncomment and run the cell below. You will be able to get the parameters for the thermal camera. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2nODFurjScB",
        "outputId": "ca916af9-b90f-43fc-8ec3-3503017d67a9"
      },
      "source": [
        "#number_rows = ?\n",
        "#number_cols = ?\n",
        "#input_folder = '/content/drive/MyDrive/3D_recon/ir_raw'\n",
        "#err,K,distCoeffs,rvecs,tvecs = get_camera_params(number_rows,number_cols,input_folder)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|█▍        | 1/7 [00:00<00:00,  8.15it/s]\u001b[A\n",
            " 29%|██▊       | 2/7 [00:00<00:00,  8.61it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/3D_recon/ir_raw/DJI_0699_R_thermal_gray.JPG : fail\n",
            "/content/drive/MyDrive/3D_recon/ir_raw/DJI_0375_R_thermal_gray_r.JPG : succeed to find corner\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 43%|████▎     | 3/7 [00:00<00:00,  8.58it/s]\u001b[A\n",
            " 71%|███████▏  | 5/7 [00:00<00:00,  9.41it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/3D_recon/ir_raw/DJI_0425_R_thermal_gray_r.JPG : succeed to find corner\n",
            "/content/drive/MyDrive/3D_recon/ir_raw/DJI_0363_R_thermal_gray_r.JPG : succeed to find corner\n",
            "/content/drive/MyDrive/3D_recon/ir_raw/DJI_0199_R_thermal_gray.JPG : fail\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 86%|████████▌ | 6/7 [00:01<00:00,  3.11it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/3D_recon/ir_raw/DJI_0427_R_thermal_gray_r.JPG : succeed to find corner\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 7/7 [00:01<00:00,  3.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/3D_recon/ir_raw/DJI_0407_R_thermal_gray_r.JPG : succeed to find corner\n",
            "Re-projection error:  0.12830457417128488\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRb1YGX_kaLT"
      },
      "source": [
        "#if you are interested to check out the camera parameters, uncomment the \n",
        "#parameters to print them out.\n",
        "\n",
        "#print(\"Re-projection error:\", err)\n",
        "#print(\"Camera matrix: \", K)\n",
        "#print(\"Distortion coefficients: \", distCoeffs)\n",
        "#print(\"Rotation matrix\", rvecs)\n",
        "#print(\"Translation matrix\", tvecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Z7vH3OYeG8"
      },
      "source": [
        "## **2 Epipolar Geometry**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJuF6YWoNHVP"
      },
      "source": [
        "#### How is the Fundamental matrix useful:\n",
        "if we know the Fundamental matrix, then simply knowing a point in an image\n",
        "gives us an easy constraint of the corresponding point in the other image. \n",
        "\n",
        "Therefore, without knowing the actual position of $P$ in $3D$ space, or any of the extrinsic or intrinsic characteristics of the cameras, we\n",
        "can establish a relationship between any $P$ and $P_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0FLjhL8Y_aE"
      },
      "source": [
        "Algorithm\n",
        "\n",
        "*   The Eight-Point Algorithm\n",
        "*   The Normalized Eight-Point Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZhNiUAceh0z"
      },
      "source": [
        "<a id='8_point'></a>\n",
        "### 2.1 The Eight-Point Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIvE60ce3IJ"
      },
      "source": [
        "<a id='normalized_8'></a>\n",
        "### 2.2 The Normalized Eight-Point Algorithm"
      ]
    }
  ]
}